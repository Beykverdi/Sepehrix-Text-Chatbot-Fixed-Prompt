# A Text Chatbot with a Fixed User Prompt and a Specified Language Model for Generating Responses
# The completion is generated by the Groq API 

#Import necessary libraries
import os
import logging
from groq import Groq
from dotenv import load_dotenv

#Define functions 
def get_groq_client():
    load_dotenv()
    api_key = os.environ.get("GROQ_API_KEY")
    # api_key = os.getenv(key="GROQ_API_KEY")   # Alternative way to get the environment variable GROQ_API_KEY
    if not api_key:
        raise ValueError("GROQ_API_KEY environment variable is not set")
    return Groq(api_key=api_key)

def generate_completion(client, prompt):
    try:
        chat_completion = client.chat.completions.create(
            messages=[
                {             
                  "role": "user",
                  "content": prompt,
                }             
         ],
            model="llama-3.3-70b-versatile", 
    )
        return chat_completion.choices[0].message.content
    except Exception as e:
        logging.error(f"Error generating completion: {e}")
        return None

def main():
    os.system(command="cls")
    client = get_groq_client()
    # load_dotenv()          # Alternative Approach to Implement get_groq_client() Function
    # client = Groq()        # Alternative Approach to Implement get_groq_client() Function    
    prompt = "Explain the importance of fast language models"
    completion = generate_completion(client, prompt)
    if completion:
        print(completion)

# Operation

if __name__ == "__main__":

    main()
